================================================================================
LLM ANNOTATION SUMMARY (for midterm report)
================================================================================

[1] DATASET OVERVIEW
--------------------------------------------------
Total Datasets: 3
Total Samples: 32,399
Model: Qwen2.5-7B-Instruct
Annotation Strategy: Two-stage (label+confidence, selective rationale)

[2] PERFORMANCE SUMMARY
----------------------------------------------------------------------
Dataset      Samples  Accuracy   F1-Score   Kappa    Confidence  
----------------------------------------------------------------------
IMDB         12,500   95.7%      0.957      0.915    0.873       
FEVER        9,999    53.8%      0.543      0.308    0.615       
Jigsaw       9,900    76.8%      0.323      0.228    0.836       

[3] TASK-SPECIFIC ANALYSIS
--------------------------------------------------

IMDB (Sentiment Analysis)
  - Accuracy: 95.7%
  - F1-Score: 0.957
  - Cohen's Kappa: 0.915
  - Average Confidence: 0.873
  - Label Distribution:
    * negative: LLM=6,286 (51.0%) vs Gold=6,213 (50.4%)
    * positive: LLM=6,033 (49.0%) vs Gold=6,106 (49.6%)
  - Confidence Distribution:
    * low: 9 (0.1%)
    * medium: 1,094 (8.9%)
    * high: 11,216 (91.0%)
  - IMDB LLM Annotation Failure Rate: 1.45%

FEVER (Fact Verification)
  - Accuracy: 53.8%
  - F1-Score: 0.543
  - Cohen's Kappa: 0.308
  - Average Confidence: 0.615
  - Label Distribution:
    * NOT ENOUGH INFO: LLM=4,120 (41.2%) vs Gold=3,333 (33.3%)
    * refutes: LLM=3,340 (33.4%) vs Gold=3,333 (33.3%)
    * supports: LLM=2,539 (25.4%) vs Gold=3,333 (33.3%)
  - Confidence Distribution:
    * low: 4,440 (44.4%)
    * medium: 603 (6.0%)
    * high: 4,956 (49.6%)

Jigsaw (Toxicity Detection)
  - Accuracy: 76.8%
  - F1-Score: 0.323
  - Cohen's Kappa: 0.228
  - Average Confidence: 0.836
  - Label Distribution:
    * non-toxic: LLM=7,307 (73.8%) vs Gold=9,109 (92.0%)
    * toxic: LLM=2,593 (26.2%) vs Gold=791 (8.0%)
  - Confidence Distribution:
    * low: 321 (3.2%)
    * medium: 1,485 (15.0%)
    * high: 8,094 (81.8%)

[4] QUALITY ASSESSMENT
--------------------------------------------------
Validation Pass Rates (JSON parsing success):
  - IMDB: 100.0% (12,319/12,319)
  - FEVER: 100.0% (9,999/9,999)
  - Jigsaw: 100.0% (9,900/9,900)

Hard Cases Identified:
  - IMDB: 530 cases (4.2%)
  - FEVER: 6,552 cases (65.5%)
  - Jigsaw: 2,580 cases (26.1%)

[5] TECHNICAL SPECIFICATIONS
--------------------------------------------------
Model Configuration:
  - Model: Qwen2.5-7B-Instruct (7B parameters)
  - Quantization: None (full precision)
  - Attention: SDPA (PyTorch native optimized)
  - Precision: bfloat16 with TF32 acceleration
  - Batch Size: 32 samples
  - Max Tokens: 24 (FEVER), 12 (IMDB/Jigsaw)
  - Temperature: 0.1 (near-deterministic)
  - Top-p: 0.7, Top-k: 10

Optimization Features:
  - Input truncation (head+tail to 512 tokens)
  - Length-based batching
  - Mixed precision inference
  - Memory management with explicit cleanup
  - Chunked processing (500 samples per chunk)
  - Checkpointing every 100 samples

[6] STATISTICAL SIGNIFICANCE
--------------------------------------------------
Macro Averages:
  - Accuracy: 75.5% ± 17.1%
  - F1-Score: 0.607 ± 0.263
  - Cohen's Kappa: 0.483 ± 0.307

